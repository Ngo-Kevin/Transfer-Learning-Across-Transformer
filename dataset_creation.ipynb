{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to flores101 training dataset folder\n",
    "train_loc = r'flores101_dataset_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'en-et': {'en':[],\n",
    "                  'et':[]},\n",
    "        'en-mk': {'en':[],\n",
    "                  'mk':[]},\n",
    "        'et-mk': {'et':[],\n",
    "                  'mk':[]}}\n",
    "\n",
    "# read files into dictionary\n",
    "for file in [x for x in os.listdir(train_loc) if x.split('.')[1] in ['en-et', 'en-mk', 'et-mk'] and x.split('.')[2] in ['en','et','mk']]:\n",
    "    with open(os.path.join(train_loc,file), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        ftype = file.split('.')\n",
    "        cleaned = [l.strip('\\n') for l in lines if l != '\\n']\n",
    "        data[ftype[1]][ftype[2]] += cleaned\n",
    "        \n",
    "    # print progress\n",
    "    print(len(data['en-et']['en']),len(data['en-et']['et']),\n",
    "          len(data['en-mk']['en']),len(data['en-mk']['mk']),\n",
    "          len(data['et-mk']['et']),len(data['et-mk']['mk']),\n",
    "          end=' '*50+'\\r')\n",
    "\n",
    "# connect lang1 text with lang2 text\n",
    "main = {}\n",
    "for l1, l2 in [('en','et'),('en','mk'),('et','mk')]:\n",
    "    reform = []\n",
    "    for lang1, lang2 in zip(data['{}-{}'.format(l1,l2)][l1],data['{}-{}'.format(l1,l2)][l2]):\n",
    "        reform.append({l1:lang1, l2:lang2})\n",
    "    main['{}-{}'.format(l1,l2)] = reform\n",
    "\n",
    "# create Dataset from dictionary\n",
    "dataset_et_mk = Dataset.from_dict({'translation':main['et-mk']})\n",
    "\n",
    "dataset_en_mk = Dataset.from_dict({'translation':main['en-mk']})\n",
    "\n",
    "# split en_et into two parts because of large size\n",
    "split = 2\n",
    "idx = int( len(main['en-et'])/ split )\n",
    "\n",
    "dataset_en_et = Dataset.from_dict({'translation':main['en-et'][:idx]})\n",
    "dataset_en_et1 = Dataset.from_dict({'translation':main['en-et'][idx:]})\n",
    "\n",
    "dataset_en_et = concatenate_datasets([dataset_en_et, dataset_en_et1])\n",
    "\n",
    "# create one Dataset containing all training examples\n",
    "d = DatasetDict()\n",
    "d['et-mk'] = dataset_et_mk\n",
    "d['en-mk'] = dataset_en_mk\n",
    "d['en-et'] = dataset_en_et\n",
    "\n",
    "d.save_to_disk(\"dataset_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to flores101 dev dataset folder\n",
    "test_dev = r'flores101_dataset_dev\\dev'\n",
    "\n",
    "# path to flores101 test dataset folder\n",
    "test_devtest = r'flores101_dataset_dev\\devtest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files into dictionary\n",
    "test_data = {'dev':{}, 'devtest':{} }\n",
    "for folder in [test_dev, test_devtest]:\n",
    "    for file in [x for x in os.listdir(folder) if x.split('.')[0] in ['eng','est','mkd']]:\n",
    "        print(file)\n",
    "        with open(os.path.join(folder,file), encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            language = file.split('.')[0]\n",
    "            cleaned = [l.replace('\\n','') for l in lines if l != '\\n']\n",
    "            test_data[folder.split('\\\\')[1]][language] = cleaned\n",
    "\n",
    "# connect lang1 text with lang2 text\n",
    "test_main = {}\n",
    "for types, v in test_data.items():\n",
    "    reform = []\n",
    "    for en, et, mk in zip(v['eng'], v['est'],v ['mkd']):\n",
    "        reform.append({'en':en, 'et':et, 'mk':mk})\n",
    "    test_main[types] = reform\n",
    "len(test_main['dev']), len(test_main['devtest'])\n",
    "\n",
    "# create Dataset from dictionary\n",
    "dataset_dev = Dataset.from_dict({'translation':test_main['dev']})\n",
    "dataset_devtest = Dataset.from_dict({'translation':test_main['devtest']})\n",
    "\n",
    "# create one Dataset containing all validation and test examples\n",
    "d_dev = DatasetDict()\n",
    "d_dev['dev'] = dataset_dev\n",
    "d_dev['devtest'] = dataset_devtest\n",
    "d_dev.save_to_disk(\"dataset_dev\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
