{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8799,
     "status": "ok",
     "timestamp": 1627227547708,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "ml666lB-jYDd",
    "outputId": "a2207c73-6474-4d20-b170-85342a20aec6"
   },
   "outputs": [],
   "source": [
    "! pip install -q sacrebleu sentencepiece transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4921,
     "status": "ok",
     "timestamp": 1627227552626,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "9yZ8Dg4XAGK1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, AdamW\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46oE1g79AGK3"
   },
   "source": [
    "# Create or Load new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(sentencepiece_path):\n",
    "    new_tokenizer_t5 = T5Tokenizer.from_pretrained(sentencepiece_path, extra_ids=0)\n",
    "    lang_code = ['__af__','__am__','__ar__','__as__','__ast__','__ay__','__az__','__ba__','__be__','__bg__','__bn__','__br__','__bs__','__ca__','__ceb__','__cjk__','__cs__','__cy__','__da__','__de__','__dyu__','__el__','__en__','__es__','__et__','__fa__','__ff__','__fi__','__fr__','__fy__','__ga__','__gd__','__gl__','__gu__','__ha__','__he__','__hi__','__hr__','__ht__','__hu__','__hy__','__id__','__ig__','__ilo__','__is__','__it__','__ja__','__jv__','__ka__','__kac__','__kam__','__kea__','__kg__','__kk__','__km__','__kmb__','__kmr__','__kn__','__ko__','__ku__','__ky__','__lb__','__lg__','__ln__','__lo__','__lt__','__luo__','__lv__','__mg__','__mi__','__mk__','__ml__','__mn__','__mr__','__ms__','__mt__','__my__','__ne__','__nl__','__no__','__ns__','__ny__','__oc__','__om__','__or__','__pa__','__pl__','__ps__','__pt__','__qu__','__ro__','__ru__','__sd__','__shn__','__si__','__sk__','__sl__','__sn__','__so__','__sq__','__sr__','__ss__','__su__','__sv__','__sw__','__ta__','__te__','__tg__','__th__','__ti__','__tl__','__tn__','__tr__','__uk__','__umb__','__ur__','__uz__','__vi__','__wo__','__xh__','__yi__','__yo__','__zh__','__zu__']\n",
    "    new_tokenizer_t5.add_special_tokens(\n",
    "        {\"additional_special_tokens\":lang_code,\n",
    "          'bos_token': '<s>',\n",
    "          'sep_token': '</s>'\n",
    "        })\n",
    "    return new_tokenizer_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    new_tokenizer_t5 = T5Tokenizer.from_pretrained(\"mt3_tokenizer\")\n",
    "except:\n",
    "    new_tokenizer_t5 = create_tokenizer('r/flores101_mm100_175M/sentencepiece.bpe.model')\n",
    "    new_tokenizer_t5.save_pretrained('mt3_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP1UME54jYDj"
   },
   "source": [
    "# Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "k0JsBMxCAGK8"
   },
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3462,
     "status": "ok",
     "timestamp": 1625948896980,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "00270532734563954145"
     },
     "user_tz": 420
    },
    "id": "6SLTUFsoAGK8",
    "outputId": "7c9ef261-7e3d-4b4a-92a8-720765fe782b"
   },
   "outputs": [],
   "source": [
    "training_data = DatasetDict.load_from_disk(\"dataset_train\")\n",
    "size_single_direction = 20000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "UaJAoaKbAGK8"
   },
   "source": [
    "### Filtering training dataset before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhqeW2g-AGK8"
   },
   "outputs": [],
   "source": [
    "def preprocess_filter(example):\n",
    "    # filter difference length\n",
    "    filter_value = {'en-mk':5,\n",
    "                    'en-et':6,\n",
    "                    'et-mk':5,}\n",
    "    \n",
    "    lang1 = example[\"translation\"][source_lang]\n",
    "    lang2 = example[\"translation\"][target_lang]\n",
    "    filter_val = max(filter_value.get(f'{source_lang}-{target_lang}',0), \n",
    "                     filter_value.get(f'{target_lang}-{source_lang}',0))\n",
    "    filtered = not( (len(lang1) < 3 or len(lang2) < 3 ) and (np.abs(len(lang1) - len(lang2)) > filter_val) )\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def create_filtered_dataset(source_lang, target_lang, dataset, preprocessing_func, batch_size, size=None):\n",
    "    np.random.seed(42)\n",
    "    language_pair = f'{source_lang}-{target_lang}'\n",
    "    \n",
    "    if size is None:\n",
    "        return dataset[language_pair].filter(preprocessing_func, batch_size=batch_size)\n",
    "    else:\n",
    "        return dataset[language_pair].select(np.random.randint(0,len(dataset[language_pair]),size=int(size*1.2))).filter(preprocessing_func, batch_size=batch_size).select(range(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "FeXd3N9pAGK9"
   },
   "source": [
    "#### Pair 1: English and Macedonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f8ebfb53a4954fb0a6fc67d493a8c62f",
      "3c49460a2a1645bf85522cfbf8184db3"
     ]
    },
    "id": "0WddW6oFAGK9",
    "outputId": "d7092a6a-ef0a-4905-f98d-ebf7c0567816"
   },
   "outputs": [],
   "source": [
    "source_lang = 'en'\n",
    "target_lang = 'mk'\n",
    "\n",
    "en_mk_filtered = create_filtered_dataset(source_lang=source_lang, \n",
    "                                         target_lang=target_lang, \n",
    "                                         dataset=training_data, \n",
    "                                         preprocessing_func=preprocess_filter, \n",
    "                                         batch_size=batch_size,\n",
    "                                         size=size_single_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "9ynkcbRAAGK9"
   },
   "source": [
    "#### Pair 2: English and Estonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "18eb9c8434874788ae3203b0d13679f6",
      "8e7566911cc54db8aefd4d02fab7cd85"
     ]
    },
    "id": "y-CUzALCAGK9",
    "outputId": "43920706-9dfe-4b3c-dfc1-29f1b2b7929d"
   },
   "outputs": [],
   "source": [
    "source_lang = 'en'\n",
    "target_lang = 'et'\n",
    "\n",
    "en_et_filtered = create_filtered_dataset(source_lang=source_lang, \n",
    "                                         target_lang=target_lang, \n",
    "                                         dataset=training_data, \n",
    "                                         preprocessing_func=preprocess_filter, \n",
    "                                         batch_size=batch_size,\n",
    "                                         size=size_single_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "AtY1mDkrAGK-"
   },
   "source": [
    "#### Pair 3: Estonian and Macedonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "5f9ee98d8690482b84b670cdb4c0479d",
      "b1d4ebdf078149eb8c66bf6a822f5e58"
     ]
    },
    "id": "bvUHL2YZAGK-",
    "outputId": "d4dd9684-ab58-465b-d236-6374c4e899cd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_lang = 'et'\n",
    "target_lang = 'mk'\n",
    "\n",
    "et_mk_filtered = create_filtered_dataset(source_lang=source_lang, \n",
    "                                         target_lang=target_lang, \n",
    "                                         dataset=training_data, \n",
    "                                         preprocessing_func=preprocess_filter, \n",
    "                                         batch_size=batch_size,\n",
    "                                         size=size_single_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "shM5i11qAGK-"
   },
   "source": [
    "#### Combining filtered datasets and creating both-ways translation pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Bj1mi9ZAGK-"
   },
   "outputs": [],
   "source": [
    "def create_two_way_dataset(dataset):\n",
    "    result = []\n",
    "    for elem in dataset['translation']:\n",
    "        l1, l2 = elem.keys()\n",
    "        s1, s2 = elem.values()\n",
    "        temp = {}\n",
    "        temp['source_'+l1] = s1\n",
    "        temp['target_'+l2] = s2\n",
    "        result.append(temp)\n",
    "        temp = {}\n",
    "        temp['source_'+l2] = s2\n",
    "        temp['target_'+l1] = s1\n",
    "        result.append(temp)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__o3dfYUAGK-"
   },
   "outputs": [],
   "source": [
    "et_mk_filtered_2way = create_two_way_dataset(et_mk_filtered)\n",
    "en_et_filtered_2way = create_two_way_dataset(en_et_filtered)\n",
    "en_mk_filtered_2way = create_two_way_dataset(en_mk_filtered)\n",
    "\n",
    "combined_training = Dataset.from_dict({'translation': et_mk_filtered_2way + en_et_filtered_2way + en_mk_filtered_2way})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "ZuRMuA5rAGK_"
   },
   "source": [
    "## Preparing validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DumhJyqgAGK_"
   },
   "outputs": [],
   "source": [
    "test_data = DatasetDict.load_from_disk(\"/content/drive/MyDrive/W266/dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9fqFuUgAGK_"
   },
   "outputs": [],
   "source": [
    "def create_two_way_dataset_val(dataset):\n",
    "    result = []\n",
    "    for elem in dataset['translation']:\n",
    "        l1, l2, l3 = elem.keys()\n",
    "        s1, s2, s3 = elem.values()\n",
    "        \n",
    "        temp = {}\n",
    "        temp['source_'+l1] = s1\n",
    "        temp['target_'+l2] = s2\n",
    "        result.append(temp)\n",
    "        \n",
    "        temp = {}\n",
    "        temp['source_'+l2] = s2\n",
    "        temp['target_'+l1] = s1\n",
    "        result.append(temp)\n",
    "        \n",
    "        temp = {}\n",
    "        temp['source_'+l2] = s2\n",
    "        temp['target_'+l3] = s3\n",
    "        result.append(temp)\n",
    "        \n",
    "        temp = {}\n",
    "        temp['source_'+l3] = s3\n",
    "        temp['target_'+l2] = s2\n",
    "        result.append(temp)\n",
    "        \n",
    "        temp = {}\n",
    "        temp['source_'+l1] = s1\n",
    "        temp['target_'+l3] = s3\n",
    "        result.append(temp)\n",
    "        \n",
    "        temp = {}\n",
    "        temp['source_'+l3] = s3\n",
    "        temp['target_'+l1] = s1\n",
    "        result.append(temp)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkUPrsrpAGK_"
   },
   "outputs": [],
   "source": [
    "val_2way = create_two_way_dataset_val(test_data['dev'])\n",
    "combined_validation = Dataset.from_dict({'translation': val_2way})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "r_ACgeGZAGLA"
   },
   "source": [
    "## Preprocessing: adding appropriate prefix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar-i1Mr4AGLA"
   },
   "outputs": [],
   "source": [
    "def get_prefix(source_lang, target_lang):\n",
    "    if source_lang == 'en' and target_lang == 'mk':\n",
    "        return 'translate English to Macedonian: '\n",
    "    elif source_lang == 'mk' and target_lang == 'en':\n",
    "        return 'translate Macedonian to English: '\n",
    "    elif source_lang == 'et' and target_lang == 'mk':\n",
    "        return 'translate Estonian to Macedonian: '\n",
    "    elif source_lang == 'mk' and target_lang == 'et':\n",
    "        return 'translate Macedonian to Estonian: '\n",
    "    elif source_lang == 'en' and target_lang == 'et':\n",
    "        return 'translate English to Estonian: '\n",
    "    else:\n",
    "        return 'translate Estonian to English: '\n",
    "\n",
    "def get_prefix_input_output(dict_obj):\n",
    "    langs, sent = [], []\n",
    "    for k, v in dict_obj.items():\n",
    "        if v is not None:\n",
    "            langs.append(k.split('_')[-1])\n",
    "            sent.append(v)\n",
    "    prefix = get_prefix(source_lang=langs[0], target_lang=langs[1])\n",
    "    return prefix, sent[0], sent[1]\n",
    "\n",
    "def preprocess_function(examples, max_input_length=50, max_target_length=50):    \n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    # append prefix\n",
    "    for element in examples['translation']:\n",
    "        prefix, source_sent, target_sent = get_prefix_input_output(element)\n",
    "        inputs.append(prefix + source_sent)\n",
    "        targets.append(target_sent)\n",
    "    \n",
    "    # get tokenized input\n",
    "    model_inputs = new_tokenizer_t5(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "    \n",
    "    # get tokenized target\n",
    "    with new_tokenizer_t5.as_target_tokenizer():\n",
    "        labels = new_tokenizer_t5(targets, max_length=max_target_length, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "tvqExa4bAGLB"
   },
   "source": [
    "### Applying preprocessing and tokenization on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "9c7ee7834b454134a2af2824f6b14626",
      "aad8a45793054725b5abd77d29774c52",
      "16b9b14180694e4e9af6f5f2ae355a6e",
      "aacfd82fe0ae473d925ce8d2c9ec3635",
      "04d96f79c0a3477db79741f74a516ba1",
      "ffaceb14b9014c9b967147d3af539dbb",
      "5a93c1fd6d834da491103b7d86293ea1",
      "0229c51da7304c60b1afa396dc08bede"
     ]
    },
    "executionInfo": {
     "elapsed": 4849,
     "status": "ok",
     "timestamp": 1626575930274,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "00270532734563954145"
     },
     "user_tz": 420
    },
    "id": "VDeXYw5CAGLC",
    "outputId": "9baf967e-b571-4386-ae6d-8b3fdc29a2d4"
   },
   "outputs": [],
   "source": [
    "tokenized_trainingset = combined_training.map(preprocess_function, batched=True)\n",
    "tokenized_devset = combined_validation.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "YnscE6OQjYDn"
   },
   "source": [
    "### Save to Processed Dataset to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix3mp8snjYDn"
   },
   "outputs": [],
   "source": [
    "tokenized_trainingset.save_to_disk(f\"trainingset_s_{size_single_direction}\")\n",
    "tokenized_devset.save_to_disk(f\"devset_s_{size_single_direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLdLUQF8Z1Qj"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eOxJ7m3jYDn"
   },
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "new_tokenizer_t5 = T5Tokenizer.from_pretrained(\"mt3_tokenizer\")\n",
    "\n",
    "# Load Dataset\n",
    "size_single_direction = 20000\n",
    "tokenized_trainingset = Dataset.load_from_disk(f\"trainingset_s_{size}\")\n",
    "tokenized_devset = Dataset.load_from_disk(f\"tokenized_devset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1627227595189,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "a7mlivEtAGLC"
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = new_tokenizer_t5.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    decoded_labels = new_tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"], \"gen_len\": result['sys_len']/len(decoded_preds),\"ref_len\": result['ref_len']/len(decoded_preds)}\n",
    "\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0Zi8XO5AGK7"
   },
   "source": [
    "## Training the model - overall set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "id": "46oE1g79AGK3"
   },
   "source": [
    "### Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(pt_embedding_model_path, randomize_embedding=True, randomize_weights=True, encoder_decoder='t5-small'):\n",
    "\n",
    "    # model checkpoints\n",
    "    model_cp_t5 = 't5-small'\n",
    "    model_cp_m2m = embedding_model_folder\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Loading the base models\n",
    "    model_m2m = torch.load(model_cp_m2m, map_location=device)\n",
    "    model_t5 = AutoModelForSeq2SeqLM.from_pretrained(model_cp_t5)\n",
    "    \n",
    "    # Randomize T5 Weights\n",
    "    if randomize_weights:\n",
    "        model_t5 = T5ForConditionalGeneration(model_t5.config)\n",
    "        \n",
    "    # Update model embedding size\n",
    "    new_embedding_size = new_tokenizer_t5.vocab_size + len(new_tokenizer_t5.additional_special_tokens) + 1 # padding token\n",
    "    model_t5.resize_token_embeddings(new_embedding_size)\n",
    "\n",
    "    # Create new Embedding structure in line with m2m-100 model\n",
    "    new_t5_embedding = torch.nn.Embedding(new_embedding_size, 512)\n",
    "    \n",
    "    # Optional: Extract the embeddings from m2m 100\n",
    "    if not randomize_embedding:\n",
    "        m2m_embeddings = model_m2m['model']['encoder.embed_tokens.weight']\n",
    "        m2m_size = m2m_embeddings.shape[0]\n",
    "        m2m_embedding_dim = m2m_embeddings.shape[1]\n",
    "\n",
    "        for i in range(m2m_size):\n",
    "            new_t5_embedding.weight.data[i] = m2m_embeddings[i]\n",
    "\n",
    "    # Update the t5 model layers \n",
    "    model_t5.shared.weight = new_t5_embedding.weight\n",
    "\n",
    "    # Update model config\n",
    "    model_t5.config.__dict__['vocab_size'] = m2m_size\n",
    "    model_t5.config.__dict__['_name_or_path'] = 'mt3_pt'\n",
    "\n",
    "    model_t5.config.__dict__['decoder_start_token_id'] = new_tokenizer_t5.bos_token_id\n",
    "    model_t5.config.__dict__['pad_token_id'] = new_tokenizer_t5.pad_token_id\n",
    "    model_t5.config.__dict__['bos_token_id'] = new_tokenizer_t5.bos_token_id\n",
    "    model_t5.config.__dict__['eos_token_id'] = new_tokenizer_t5.eos_token_id\n",
    "    model_t5.config.__dict__['task_specific_params'] = {\n",
    "        'translation_en_to_et': {'early_stopping': True,\n",
    "              'max_length': 300,\n",
    "              'num_beams': 4,\n",
    "              'prefix': 'translate English to Estonian: '},\n",
    "\n",
    "         'translation_en_to_mk': {'early_stopping': True,\n",
    "              'max_length': 300,\n",
    "              'num_beams': 4,\n",
    "              'prefix': 'translate English to Macedonian: '},\n",
    "         'translation_et_to_en': {'early_stopping': True,\n",
    "\n",
    "              'max_length': 300,\n",
    "              'num_beams': 4,\n",
    "              'prefix': 'translate Estonian to English: '},\n",
    "         'translation_et_to_mk': {'early_stopping': True,\n",
    "\n",
    "              'max_length': 300,\n",
    "              'num_beams': 4,\n",
    "              'prefix': 'translate Estonian to Macedonian: '},\n",
    "\n",
    "         'translation_mk_to_en': {'early_stopping': True,\n",
    "              'max_length': 300,\n",
    "              'num_beams': 4,\n",
    "              'prefix': 'translate Macedonian to English: '},\n",
    "\n",
    "         'translation_mk_to_et': {'early_stopping': True,\n",
    "              'max_length': 300,\n",
    "              'num_beams': 4,\n",
    "              'prefix': 'translate Macedonian to Estonian: '}}\n",
    "    \n",
    "    return model_t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uqtLdd8AGK7"
   },
   "source": [
    "### overall set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5 = create_model(\"r/flores101_mm100_175M/model.pt\", randomize_embedding=True, randomize_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1627227573180,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "fPNNYMT9AGK7"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "591cc6fc2b4e4f9bb9ebcc5da5dbbad6",
      "ee4ffeccbf884ceb985a33aa58fcf006",
      "33deec9efd4c45c099a262fa352b88ce",
      "81d655d0f26948058c043a2cc0a821e1",
      "68a39bf78f604f83a96ad40a2f5e949d",
      "d98ebd2d89c94dfa9dfeb73304951b41",
      "0e28b53c2ae64ecd9a84cc797d1e59ff",
      "72a3f7ac80984fffb849598c0495ad50",
      "dcefa1a3671440dbb9d22c86e47c4817",
      "a73d0aec0c3a476a8ec7be3f2863747e",
      "09b6f0e9933c4908937796321bb558f0"
     ]
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1627227574098,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "82ON5sxSAGK7",
    "outputId": "8616a04c-e351-4014-f03f-79fafc8f8b14"
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1627227574099,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "frdANqkwAGK7",
    "outputId": "83c7dd8b-68ef-44ab-9980-66da43cf190c"
   },
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/test-translation\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    logging_strategy = 'epoch',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=30,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'bleu',\n",
    "    predict_with_generate=True,\n",
    "    save_strategy= \"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1627227595190,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "Er2Kx3WAAGLC"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(new_tokenizer_t5, model=model_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6481,
     "status": "ok",
     "timestamp": 1627227601668,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "nFry2qbZAGLC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_t5,\n",
    "    args,\n",
    "    train_dataset=tokenized_trainingset,\n",
    "    eval_dataset=tokenized_devset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=new_tokenizer_t5,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "error",
     "timestamp": 1627273505204,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "v_BhTXil8D1J"
   },
   "outputs": [],
   "source": [
    "# continuing from cp\n",
    "train_cp = \"\"\n",
    "if train_cp:\n",
    "    trainer.train(train_cp)    \n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3943,
     "status": "ok",
     "timestamp": 1627266460577,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "m_U0EXuijYDo",
    "outputId": "84725d7d-8db2-4d4f-c4a4-9aeea192a2fb"
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_t5.save_pretrained(\"mode2_20k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liLVMVoAjYDr"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1627266460577,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "cv5cL3bEjYDr"
   },
   "outputs": [],
   "source": [
    "def extract_translation(examples):\n",
    "    global target_lang\n",
    "    inputs = [[ex[target_lang]] for ex in examples[\"translation\"]]\n",
    "    inputs_50 = [  [new_tokenizer_t5.decode( new_tokenizer_t5( ex[target_lang],max_length=50, truncation=True).input_ids, skip_special_tokens=True) ]  for ex in examples[\"translation\"]]\n",
    "    return inputs, inputs_50\n",
    "\n",
    "def score_translations(generalized_file_path, test_dataset_path):\n",
    "    global source_lang, target_lang\n",
    "    raw_test = DatasetDict.load_from_disk(test_dataset_path)\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    bleu_scores = {}\n",
    "    for source_lang, target_lang in permutations(['en','et','mk'], 2):\n",
    "        print(source_lang, \"-->\" ,target_lang)\n",
    "        # get predictions \n",
    "        with open(generalized_file_path.format(source_lang, target_lang), encoding=\"utf-8\") as f:\n",
    "            pred = [l.replace('\\n','') for l in f.readlines() if l != '\\n']\n",
    "        # get reference\n",
    "        ref, ref_50 = extract_translation(raw_test['devtest'])\n",
    "        # get score\n",
    "        bleu = metric.compute(predictions=pred, references=ref)\n",
    "        bleu_50 = metric.compute(predictions=pred, references=ref_50)\n",
    "\n",
    "        bleu_scores[\"{}_{}\".format(source_lang, target_lang)] = bleu\n",
    "        bleu_scores[\"{}_{}-50\".format(source_lang, target_lang)] = bleu_50\n",
    "\n",
    "    for dire in bleu_scores.keys():\n",
    "        prec = [np.round(x,5) for x in bleu_scores[dire]['precisions']]\n",
    "        bleu_scores[dire]['precisions'] = prec\n",
    "\n",
    "    return pd.DataFrame.from_dict(bleu_scores,'index')\n",
    "\n",
    "def score_translations_from_model(model, tokenizer):\n",
    "    global source_lang, target_lang\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    bleu_scores = {}\n",
    "\n",
    "    for source_lang, target_lang in permutations(['en','et','mk'], 2):\n",
    "        print(source_lang, \"-->\" ,target_lang)\n",
    "\n",
    "        # get predictions \n",
    "        if os.path.exists(f'generate.{source_lang}.{target_lang}'):\n",
    "            with open(f'generate.{source_lang}.{target_lang}', encoding=\"utf-8\") as f:\n",
    "                predictions = [l.replace('\\n','') for l in f.readlines() if l != '\\n']\n",
    "        else:\n",
    "\n",
    "            try:\n",
    "                temp = DatasetDict.load_from_disk(f\"tokenized_testset.{source_lang}.{target_lang}\")\n",
    "            except:\n",
    "                temp = test_data['devtest'].map(preprocess_function_test, batched=True)\n",
    "                temp.save_to_disk(f\"tokenized_testset.{source_lang}.{target_lang}\")\n",
    "\n",
    "            temp.set_format(type='torch', columns=['input_ids','attention_mask'])\n",
    "            train_loader = DataLoader(temp, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "            predictions, labels = [], []\n",
    "            start = time.time()\n",
    "            for step, batch in enumerate(train_loader):\n",
    "                print(step,end='\\r')\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                output = model.generate(input_ids, attention_mask=attention_mask) #, forced_bos_token_id=tokenizer.get_lang_id(target_lang) )\n",
    "                labels += tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "                predictions += tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            end = time.time()\n",
    "            print(\"time taken: \",end-start)\n",
    "\n",
    "            with open(f'generate.{source_lang}.{target_lang}','w', encoding='utf-8') as output:\n",
    "                output.write(\"\\n\".join(predictions))\n",
    "\n",
    "        labels, labels_50 = extract_translation(test_data['devtest'])\n",
    "\n",
    "        bleu = metric.compute(predictions=predictions, references=labels)\n",
    "        bleu_50 = metric.compute(predictions=predictions, references=labels_50)\n",
    "\n",
    "        bleu_scores[\"{}_{}\".format(source_lang, target_lang)] = bleu\n",
    "        bleu_scores[\"{}_{}-50\".format(source_lang, target_lang)] = bleu_50\n",
    "        print({k:v['score'] for k,v in bleu_scores.items()})\n",
    "\n",
    "    for dire in bleu_scores.keys():\n",
    "        prec = [np.round(x,5) for x in bleu_scores[dire]['precisions']]\n",
    "        bleu_scores[dire]['precisions'] = prec\n",
    "    return pd.DataFrame.from_dict(bleu_scores,'index')\n",
    "\n",
    "def get_prefix(source_lang, target_lang):\n",
    "    if source_lang == 'en' and target_lang == 'mk':\n",
    "        return 'translate English to Macedonian: '\n",
    "    elif source_lang == 'mk' and target_lang == 'en':\n",
    "        return 'translate Macedonian to English: '\n",
    "    elif source_lang == 'et' and target_lang == 'mk':\n",
    "        return 'translate Estonian to Macedonian: '\n",
    "    elif source_lang == 'mk' and target_lang == 'et':\n",
    "        return 'translate Macedonian to Estonian: '\n",
    "    elif source_lang == 'en' and target_lang == 'et':\n",
    "        return 'translate English to Estonian: '\n",
    "    else:\n",
    "        return 'translate Estonian to English: '\n",
    "\n",
    "def preprocess_function_test(examples):\n",
    "    max_input_length = 300\n",
    "\n",
    "    inputs = []\n",
    "    for element in examples['translation']:\n",
    "        prefix = get_prefix(source_lang, target_lang)\n",
    "        inputs.append(prefix + element[source_lang])\n",
    "\n",
    "    model_inputs = new_tokenizer_t5(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "  \n",
    "    return model_inputs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hNXBv8Aah0L"
   },
   "outputs": [],
   "source": [
    "# New Tokenizer\n",
    "new_tokenizer_t5 = T5Tokenizer.from_pretrained(\"mt3_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5765,
     "status": "ok",
     "timestamp": 1627266466340,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "sSk5XGn-8PbW"
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "test_data = DatasetDict.load_from_disk(\"/content/drive/MyDrive/W266/dataset_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_path = r'test-translation/checkpoint-112500'\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(cp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Score from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 298824,
     "status": "ok",
     "timestamp": 1627273814510,
     "user": {
      "displayName": "Kevin Ngo",
      "photoUrl": "",
      "userId": "17750278640470830834"
     },
     "user_tz": 420
    },
    "id": "o0VHtfWyn_E4",
    "outputId": "78249dc5-70ff-4aa1-d504-d1cbc5108d45"
   },
   "outputs": [],
   "source": [
    "score_translations_from_model(model_t5, new_tokenizer_t5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "46oE1g79AGK3",
    "gP1UME54jYDj",
    "k0JsBMxCAGK8",
    "UaJAoaKbAGK8",
    "WvhSccY0AGK9",
    "shM5i11qAGK-",
    "ZuRMuA5rAGK_",
    "r_ACgeGZAGLA",
    "tvqExa4bAGLB",
    "YnscE6OQjYDn",
    "ox6AZ4xcAGLD",
    "1I9EOoHXAGLE"
   ],
   "name": "fin_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
